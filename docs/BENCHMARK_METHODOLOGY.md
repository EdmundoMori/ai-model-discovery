# Experimentos y Evaluaci√≥n

Este directorio contendr√° los scripts de evaluaci√≥n y an√°lisis para las diferentes fases del proyecto.

## Estado: üìÖ Planificado para Fase 5 (Semana 8)

## Experimentos Planificados

### 1. Evaluaci√≥n de Text-to-SPARQL

**Archivo**: `eval_text_to_sparql.py`

M√©tricas:
- Exactitud sint√°ctica (queries v√°lidas)
- Exactitud sem√°ntica (resultados correctos)
- Cobertura de conceptos
- Tiempo de respuesta

Dataset de prueba:
- 50 consultas naturales anotadas
- Ground truth SPARQL correspondiente

### 2. Comparaci√≥n de M√©todos de B√∫squeda

**Archivo**: `compare_search_methods.py`

Comparar:
- M√©todo 1: Non-federated
- M√©todo 2: Federated
- M√©todo 3: Cross-repository

M√©tricas:
- Precision@K
- Recall@K
- F1-Score
- Latencia

### 3. An√°lisis de Cobertura

**Archivo**: `coverage_analysis.py`

Evaluar:
- % modelos con metadatos completos
- Distribuci√≥n de tareas
- Cobertura de licencias
- Calidad de mapeo a DAIMO

### 4. An√°lisis de Popularidad vs Relevancia

**Archivo**: `popularity_vs_relevance.py`

Investigar:
- Correlaci√≥n entre descargas y relevancia
- Sesgo de popularidad en ranking
- Diversidad de resultados

## Formato de Resultados

Los experimentos generar√°n:

- **CSV/JSON**: Resultados tabulados
- **Gr√°ficos**: Visualizaciones (matplotlib/seaborn)
- **Reportes**: Markdown con an√°lisis

Ejemplo:
```
experiments/
  results/
    eval_text_to_sparql_2026-01-26.json
    coverage_analysis_2026-01-26.csv
  figures/
    precision_recall_curve.png
    task_distribution.png
```

## Benchmarks

### Test Suite para Text-to-SPARQL

Categor√≠as de consultas:
1. **Simple**: Filtro por una propiedad
2. **Compuesta**: M√∫ltiples filtros
3. **Agregaci√≥n**: COUNT, AVG, etc.
4. **Comparativa**: Ranking, TOP-K
5. **Provenance**: Modelos derivados, datasets

Ejemplo:
```json
{
  "id": 1,
  "query_nl": "Top 5 modelos de clasificaci√≥n de im√°genes con licencia MIT",
  "query_sparql": "PREFIX daimo: ...",
  "difficulty": "medium",
  "expected_results": 5
}
```

## Reproducibilidad

Todos los experimentos incluir√°n:
- Seed fijo para aleatoriedad
- Versi√≥n de dependencias (Poetry lock)
- Configuraci√≥n expl√≠cita en YAML
- Scripts de ejecuci√≥n automatizados

## Referencias Acad√©micas

Los resultados se documentar√°n siguiendo est√°ndares de:
- ACL (para NLP/LLM)
- ISWC (para Semantic Web)
- MLSys (para infraestructura ML)
# üî¨ Evaluaci√≥n Acad√©mica - M√©todo 1 (Text-to-SPARQL)

Este directorio contiene el pipeline completo de evaluaci√≥n experimental para validar el M√©todo 1 con rigor acad√©mico.

## üìÅ Estructura

```
benchmarks/
‚îú‚îÄ‚îÄ EVALUATION_DESIGN.md          # Dise√±o experimental detallado
‚îú‚îÄ‚îÄ evaluation_pipeline.ipynb      # üåü NOTEBOOK PRINCIPAL - Pipeline interactivo
‚îú‚îÄ‚îÄ create_snapshot.py             # Script para crear snapshot reproducible
‚îú‚îÄ‚îÄ validate_benchmark.py          # Script para validar benchmark
‚îú‚îÄ‚îÄ run_keyword_benchmark.py       # Ejecutar baseline BM25
‚îú‚îÄ‚îÄ run_text2sparql_benchmark.py   # Ejecutar M√©todo 1
‚îú‚îÄ‚îÄ keyword_bm25.py                # Implementaci√≥n baseline
‚îú‚îÄ‚îÄ metrics.py                     # Funciones de m√©tricas
‚îú‚îÄ‚îÄ queries.jsonl                  # Benchmark original (12 queries)
‚îú‚îÄ‚îÄ queries_50.jsonl               # Benchmark expandido (50+ queries)
‚îú‚îÄ‚îÄ benchmark_schema.md            # Esquema del benchmark
‚îú‚îÄ‚îÄ snapshot/                      # Snapshot del grafo RDF
‚îÇ   ‚îú‚îÄ‚îÄ graph_snapshot.ttl         # Grafo congelado
‚îÇ   ‚îú‚îÄ‚îÄ snapshot_metadata.json     # Metadatos + SHA256
‚îÇ   ‚îî‚îÄ‚îÄ README.md                  # Documentaci√≥n del snapshot
‚îú‚îÄ‚îÄ results/                       # Resultados de experimentos
‚îÇ   ‚îú‚îÄ‚îÄ results_bm25.jsonl
‚îÇ   ‚îú‚îÄ‚îÄ report_bm25.json
‚îÇ   ‚îú‚îÄ‚îÄ results_method1_configA.jsonl
‚îÇ   ‚îú‚îÄ‚îÄ report_method1_configA.json
‚îÇ   ‚îú‚îÄ‚îÄ comparison_table.csv
‚îÇ   ‚îú‚îÄ‚îÄ statistical_tests.csv
‚îÇ   ‚îú‚îÄ‚îÄ error_analysis.csv
‚îÇ   ‚îî‚îÄ‚îÄ FINAL_REPORT.md           # üìÑ Reporte completo para paper/tesis
‚îî‚îÄ‚îÄ figures/                       # Gr√°ficos para publicaci√≥n
    ‚îú‚îÄ‚îÄ metrics_comparison.png
    ‚îú‚îÄ‚îÄ latency_comparison.png
    ‚îî‚îÄ‚îÄ performance_by_difficulty.png
```

---

## üöÄ Quickstart

### Opci√≥n 1: Notebook Interactivo (Recomendado)

El notebook `evaluation_pipeline.ipynb` contiene TODO el proceso de principio a fin con explicaciones detalladas:

```bash
# Abrir notebook en VS Code o Jupyter
code evaluation_pipeline.ipynb

# O ejecutar con Jupyter
jupyter notebook evaluation_pipeline.ipynb
```

**El notebook incluye:**
1. ‚úÖ Creaci√≥n de snapshot reproducible
2. ‚úÖ An√°lisis exploratorio del grafo
3. ‚úÖ Expansi√≥n del benchmark a 50 queries
4. ‚úÖ Validaci√≥n de ground truth
5. ‚úÖ Ejecuci√≥n autom√°tica de todos los benchmarks
6. ‚úÖ Tests estad√≠sticos (paired t-test, confidence intervals)
7. ‚úÖ An√°lisis de errores cualitativos
8. ‚úÖ Visualizaciones para paper/tesis
9. ‚úÖ Generaci√≥n de reporte final

### Opci√≥n 2: Scripts Individuales

Si prefieres ejecutar cada paso por separado:

#### Paso 1: Crear Snapshot Reproducible

```bash
python create_snapshot.py \
    --source ../../data/ai_models_multi_repo.ttl \
    --output ./snapshot
```

Esto genera:
- `snapshot/graph_snapshot.ttl` (grafo congelado)
- `snapshot/snapshot_metadata.json` (SHA256 + stats)

#### Paso 2: Expandir Benchmark (si es necesario)

Actualmente hay 12 queries. Para expandir a 50+, ejecutar las celdas correspondientes del notebook o generar manualmente.

#### Paso 3: Validar Benchmark

```bash
python validate_benchmark.py \
    --queries queries_50.jsonl \
    --graph snapshot/graph_snapshot.ttl \
    --output validation_report.json
```

#### Paso 4: Ejecutar Benchmarks

**Baseline BM25:**
```bash
python run_keyword_benchmark.py \
    --graph snapshot/graph_snapshot.ttl \
    --queries queries_50.jsonl \
    --results results/results_bm25.jsonl \
    --report results/report_bm25.json \
    --k 5
```

**M√©todo 1 - Config Principal (RAG activado):**
```bash
python run_text2sparql_benchmark.py \
    --graph snapshot/graph_snapshot.ttl \
    --queries queries_50.jsonl \
    --results results/results_method1_configA.jsonl \
    --report results/report_method1_configA.json \
    --k 5 \
    --use-rag \
    --top-k-examples 3 \
    --temperature 0.1 \
    --llm-provider ollama \
    --model deepseek-r1:7b
```

**M√©todo 1 - Config Ablation (sin RAG):**
```bash
python run_text2sparql_benchmark.py \
    --graph snapshot/graph_snapshot.ttl \
    --queries queries_50.jsonl \
    --results results/results_method1_configB.jsonl \
    --report results/report_method1_configB.json \
    --k 5 \
    --no-rag \
    --temperature 0.1
```

#### Paso 5: An√°lisis Comparativo

Los reportes JSON contienen todas las m√©tricas agregadas. Para comparaci√≥n visual, usa el notebook o crea tus propios scripts de an√°lisis.

---

## üìä M√©tricas Implementadas

### Capa 1: Validez Sint√°ctica
- **Parse Success Rate**: % queries que generan SPARQL v√°lido
- **Execution Success Rate**: % queries que ejecutan sin error
- **Coverage**: % queries con al menos 1 resultado

### Capa 2: Exactitud Sem√°ntica (Recuperaci√≥n)
- **Precision@5**: Precisi√≥n en top-5
- **Recall@5**: Cobertura en top-5
- **F1@5**: Media arm√≥nica P y R
- **NDCG@5**: Normalized Discounted Cumulative Gain
- **MRR**: Mean Reciprocal Rank
- **MAP@5**: Mean Average Precision
- **Hit@5**: % queries con ‚â•1 relevante en top-5

### Capa 3: Similitud de Conjuntos
- **Exact Match**: Set(pred) == Set(gold)
- **Jaccard**: Similitud Jaccard
- **Result Count Error**: |count(pred) - count(gold)|

### Capa 4: Eficiencia
- **Latency (avg, p95, p99)**: Tiempo total
- **Conversion Time**: Tiempo NL‚ÜíSPARQL
- **Execution Time**: Tiempo ejecuci√≥n SPARQL

---

## üî¨ Dise√±o Experimental

### Configuraciones Evaluadas

| Config | RAG | Temp | K Examples | Prop√≥sito |
|--------|-----|------|------------|-----------|
| **A**  | ‚úÖ  | 0.1  | 3          | Principal (mejor rendimiento) |
| **B**  | ‚ùå  | 0.1  | -          | Ablation: ¬øQu√© aporta RAG? |
| **C**  | ‚úÖ  | 0.5  | 3          | Ablation: ¬øEfecto temperatura? |
| **D**  | ‚úÖ  | 0.1  | 5          | Ablation: ¬øM√°s ejemplos mejor? |

### Baseline

**BM25 Keyword Search:**
- √çndice: Concatenaci√≥n de t√≠tulo, descripci√≥n, task, library, license, tags, source
- Par√°metros: k1=1.5, b=0.75 (est√°ndar)
- Input: keywords extra√≠dos de cada query

### Tests Estad√≠sticos

**Paired t-test (one-tailed):**
- H‚ÇÄ: M√©todo1 ‚â§ Baseline (no mejora)
- H‚ÇÅ: M√©todo1 > Baseline (mejora)
- Œ± = 0.05

Aplicado a: P@5, R@5, F1@5, NDCG@5, MRR

**Intervalos de Confianza:**
- 95% CI para todas las m√©tricas principales
- Reportado como: mean ¬± margin

**Effect Size:**
- Cohen's d para magnitud del efecto

---

## üìà Outputs Generados

### Reportes JSON
- `report_*.json`: M√©tricas agregadas por m√©todo
- `results_*.jsonl`: M√©tricas por query

### Tablas CSV
- `comparison_table.csv`: Comparaci√≥n entre m√©todos
- `statistical_tests.csv`: Resultados de tests estad√≠sticos
- `error_analysis.csv`: Queries problem√°ticas

### Visualizaciones PNG
- `metrics_comparison.png`: Barras comparativas
- `latency_comparison.png`: Latencias por m√©todo
- `performance_by_difficulty.png`: Boxplots por dificultad

### Reporte Final
- `FINAL_REPORT.md`: Reporte completo en Markdown listo para incluir en paper/tesis

---

## ‚úÖ Checklist de Validaci√≥n Acad√©mica

Antes de enviar paper/tesis, verificar:

### Reproducibilidad
- [ ] Snapshot del grafo con SHA256 documentado
- [ ] C√≥digo en repositorio con instrucciones claras
- [ ] requirements.txt con versiones congeladas
- [ ] Benchmark versionado (queries.jsonl)
- [ ] README explica c√≥mo reproducir

### Rigor Estad√≠stico
- [ ] n ‚â• 50 queries
- [ ] Tests de significancia ejecutados y reportados
- [ ] Intervalos de confianza incluidos
- [ ] Ablation studies completados

### Validez
- [ ] Ground truth verificado manualmente
- [ ] Baseline apropiado (BM25, no strawman)
- [ ] M√©tricas est√°ndar del campo IR/QA
- [ ] An√°lisis cualitativo de errores

### Transparencia
- [ ] Limitaciones expl√≠citas
- [ ] Casos de fallo documentados
- [ ] Hiperpar√°metros justificados
- [ ] Trade-offs discutidos

---

## üìö Referencias y Recursos

### Documentos de Dise√±o
- [EVALUATION_DESIGN.md](EVALUATION_DESIGN.md) - Dise√±o experimental completo
- [benchmark_schema.md](benchmark_schema.md) - Esquema del benchmark

### Est√°ndares Acad√©micos
- **Reproducibilidad**: ACM Artifact Review and Badging
- **M√©tricas IR**: Manning et al. "Introduction to Information Retrieval"
- **NDCG**: J√§rvelin & Kek√§l√§inen (2002)
- **Benchmarks similares**: QALD, LC-QuAD, Spider

### Papers de Referencia
- Text-to-SPARQL: Ver secci√≥n de Related Work en EVALUATION_DESIGN.md
- BM25: Robertson & Zaragoza (2009)
- Statistical testing: Field "Discovering Statistics" (2013)

---

## üêõ Troubleshooting

### Error: "Grafo no encontrado"
```bash
# Aseg√∫rate de que el grafo existe:
ls -lh ../../data/ai_models_multi_repo.ttl

# Si no existe, constr√∫yelo primero:
cd ../../knowledge_graph
python build_graph.py
```

### Error: "LLM no responde"
```bash
# Verificar que Ollama est√° corriendo:
ollama list

# Si el modelo no est√° instalado:
ollama pull deepseek-r1:7b
```

### Error: "Queries con URIs incorrectas"
```bash
# Validar benchmark:
python validate_benchmark.py \
    --queries queries_50.jsonl \
    --graph snapshot/graph_snapshot.ttl

# Revisar validation_report.json para detalles
```

### Performance lenta
- Reducir n√∫mero de queries temporalmente
- Usar modelo LLM m√°s peque√±o
- Ejecutar en m√°quina con m√°s RAM/CPU

---

## ü§ù Contribuir

Si encuentras bugs o mejoras:

1. Documentar en issue
2. Proponer fix con PR
3. Asegurar que pasa validaci√≥n

---

## üìß Contacto

**Autor:** Edmundo
**Proyecto:** AI Model Discovery
**Repositorio:** ai-model-discovery

---

## üìú Licencia

Ver LICENSE en el repositorio principal.

---

**¬°√âxito con tu investigaci√≥n! üöÄ**

Para cualquier duda, revisar primero [EVALUATION_DESIGN.md](EVALUATION_DESIGN.md) que contiene explicaciones detalladas de cada componente del proceso de evaluaci√≥n.
# Dise√±o de Evaluaci√≥n Acad√©mica para M√©todo 1 (Text-to-SPARQL)

## 1. An√°lisis de Requisitos para Validaci√≥n Acad√©mica

### 1.1 Est√°ndares para Publicaci√≥n Cient√≠fica

Para que la evaluaci√≥n del M√©todo 1 sea aceptada en un paper acad√©mico o tesis, debe cumplir con:

#### **Reproducibilidad (CRITICAL)**
- ‚úÖ Dataset fijo y versionado (snapshot del grafo RDF)
- ‚úÖ C√≥digo abierto y documentado
- ‚úÖ Hiperpar√°metros expl√≠citos y fijos
- ‚úÖ Semilla aleatoria establecida (si aplica)
- ‚úÖ Versiones de dependencias congeladas

#### **Validez Interna**
- ‚úÖ Ground truth verificado manualmente
- ‚úÖ M√©tricas est√°ndar de IR/QA
- ‚úÖ Separaci√≥n clara entre validaci√≥n sint√°ctica y sem√°ntica
- ‚úÖ An√°lisis de errores cualitativos

#### **Validez Externa**
- ‚úÖ Tama√±o de muestra justificado (n‚â•50 recomendado)
- ‚úÖ Distribuci√≥n representativa de complejidad (b√°sico/medio/avanzado)
- ‚úÖ Cobertura de tipos de consulta (filtrado, ranking, agregaci√≥n)

#### **Rigor Estad√≠stico**
- ‚úÖ Tests de significancia (si se comparan m√©todos)
- ‚úÖ Intervalos de confianza para m√©tricas
- ‚úÖ Reporte de varianza/desviaci√≥n est√°ndar
- ‚úÖ Ablation studies (efecto del RAG, temperatura, etc.)

---

## 2. Dise√±o Experimental Completo

### 2.1 Snapshot Reproducible del Grafo

**Objetivo:** Congelar el estado del grafo RDF para que todos los experimentos usen exactamente los mismos datos.

**Implementaci√≥n:**
```python
# experiments/benchmarks/create_snapshot.py
import hashlib
import json
from datetime import datetime
from pathlib import Path

def create_snapshot(source_graph: Path, output_dir: Path):
    """
    Crea un snapshot reproducible del grafo con metadatos.
    """
    # 1. Copiar grafo
    snapshot_file = output_dir / "graph_snapshot.ttl"
    snapshot_file.write_bytes(source_graph.read_bytes())
    
    # 2. Calcular hash SHA256
    sha256 = hashlib.sha256(snapshot_file.read_bytes()).hexdigest()
    
    # 3. Contar modelos (ejecutar SPARQL COUNT)
    from rdflib import Graph
    g = Graph()
    g.parse(snapshot_file, format="turtle")
    count_query = """
    PREFIX daimo: <http://purl.org/pionera/daimo#>
    SELECT (COUNT(?model) AS ?count) WHERE {
        ?model a daimo:Model .
    }
    """
    result = g.query(count_query)
    model_count = list(result)[0][0]
    
    # 4. Metadatos del snapshot
    metadata = {
        "snapshot_id": f"v1_{datetime.now().strftime('%Y%m%d')}",
        "created_at": datetime.now().isoformat(),
        "source_file": str(source_graph),
        "sha256": sha256,
        "size_bytes": snapshot_file.stat().st_size,
        "model_count": int(model_count),
        "format": "turtle",
        "ontology": "DAIMO (http://purl.org/pionera/daimo#)",
    }
    
    # 5. Guardar metadatos
    metadata_file = output_dir / "snapshot_metadata.json"
    metadata_file.write_text(json.dumps(metadata, indent=2))
    
    return metadata

# Uso:
# snapshot_info = create_snapshot(
#     source_graph=Path("data/ai_models_multi_repo.ttl"),
#     output_dir=Path("experiments/benchmarks/snapshot")
# )
```

**Verificaci√≥n de Integridad:**
```bash
# Cualquier investigador puede verificar:
sha256sum experiments/benchmarks/snapshot/graph_snapshot.ttl
# Debe coincidir con snapshot_metadata.json["sha256"]
```

---

### 2.2 Benchmark de 50 Consultas

**Distribuci√≥n por Complejidad:**
- **20 B√°sicas (40%)**: 1 filtro simple (task, library, license, source)
- **20 Intermedias (40%)**: 2-3 filtros, OPTIONAL, negaciones, order by
- **10 Avanzadas (20%)**: Agregaciones (COUNT, AVG), GROUP BY, HAVING, subconsultas

**Distribuci√≥n por Tipo de Respuesta:**
- **35 Recuperaci√≥n (70%)**: Devuelven lista de modelos
- **10 Ranking (20%)**: ORDER BY + LIMIT (top-K)
- **5 Agregaci√≥n (10%)**: COUNT, AVG, SUM, GROUP BY

**Criterios de Calidad del Ground Truth:**
1. Cada query debe tener SPARQL gold ejecutado manualmente
2. Resultados verificados contra el snapshot (no contra grafo din√°mico)
3. Anotaci√≥n expl√≠cita de dificultad y tipo
4. Keywords extra√≠dos para baseline BM25

**Template de Query:**
```json
{
  "id": "q001",
  "query_nl": "PyTorch models for image classification",
  "query_keywords": ["pytorch", "image", "classification"],
  "difficulty": "basic",
  "query_type": "retrieval",
  "gold_sparql": "PREFIX daimo: ... SELECT DISTINCT ?model WHERE {...}",
  "gold_model_uris": ["http://..."],
  "expected_count": 88,
  "notes": "Filter by task + library"
}
```

---

### 2.3 M√©tricas de Evaluaci√≥n

#### **Capa 1: Validez Sint√°ctica**
- **Parse Success Rate**: % de queries que generan SPARQL parseable
- **Execution Success Rate**: % de queries que ejecutan sin error
- **Query Safety**: % sin operaciones peligrosas (UPDATE, DELETE, DROP)

#### **Capa 2: Exactitud Sem√°ntica**

**Para consultas de recuperaci√≥n:**
- **Exact Match**: Set(predicted) == Set(gold) ‚Üí {0, 1}
- **Jaccard Similarity**: |A‚à©B| / |A‚à™B|
- **Precision@5**: |topK(pred) ‚à© gold| / K
- **Recall@5**: |topK(pred) ‚à© gold| / |gold|
- **F1@5**: Harmonic mean of P@5 and R@5
- **NDCG@5**: Normalized Discounted Cumulative Gain (con relevancia binaria)
- **MAP@5**: Mean Average Precision
- **MRR**: Mean Reciprocal Rank
- **Hit@5**: ¬øAl menos 1 relevante en top-5?

**Para consultas de agregaci√≥n:**
- **Numeric Accuracy**: valor_pred == valor_gold ‚Üí {0, 1}
- **Relative Error**: |pred - gold| / gold (si gold ‚â† 0)

#### **Capa 3: Eficiencia Operacional**
- **Latency (avg, median, p95, p99)**: Tiempo total NL‚ÜíSPARQL‚Üíresultados
- **Conversion Time**: Tiempo solo de NL‚ÜíSPARQL
- **Execution Time**: Tiempo solo de ejecuci√≥n SPARQL

---

### 2.4 Baseline para Comparaci√≥n

**Baseline BM25 (Keyword Search):**
- **Algoritmo**: BM25 sobre texto concatenado (t√≠tulo, descripci√≥n, tags, task, library, license, source)
- **Par√°metros fijos**: k1=1.5, b=0.75 (valores est√°ndar)
- **Input**: `query_keywords` del benchmark
- **Output**: Top-K modelos rankeados por score BM25

**Justificaci√≥n:**
- BM25 es el est√°ndar de facto en IR para baselines no supervisados
- No requiere entrenamiento ‚Üí reproducible
- Usa las mismas queries (fairness)
- Permite demostrar ventaja del enfoque sem√°ntico sobre keyword matching

---

### 2.5 Configuraciones Experimentales (Ablation Studies)

Para responder preguntas de investigaci√≥n clave, ejecutar el M√©todo 1 en 4 configuraciones:

| Config | RAG Enabled | Temperature | k_examples | Objetivo |
|--------|-------------|-------------|------------|----------|
| **A**  | ‚úÖ S√≠       | 0.1         | 3          | **Principal** (mejor rendimiento esperado) |
| **B**  | ‚ùå No       | 0.1         | -          | Ablation: ¬øQu√© aporta el RAG? |
| **C**  | ‚úÖ S√≠       | 0.5         | 3          | Ablation: ¬øEfecto de temperatura? |
| **D**  | ‚úÖ S√≠       | 0.1         | 5          | Ablation: ¬øM√°s ejemplos mejoran? |

**An√°lisis Esperado:**
- Comparar A vs B ‚Üí Ganancia del RAG
- Comparar A vs C ‚Üí Efecto de temperatura (exploraci√≥n vs determinismo)
- Comparar A vs D ‚Üí Efecto del n√∫mero de ejemplos

---

## 3. Protocolo de Ejecuci√≥n

### 3.1 Preparaci√≥n

1. **Crear snapshot reproducible:**
   ```bash
   python create_snapshot.py
   ```

2. **Validar benchmark:**
   ```bash
   python validate_benchmark.py --queries queries.jsonl --graph snapshot/graph_snapshot.ttl
   # Debe verificar:
   # - Todos los gold_sparql son v√°lidos
   # - Todos los gold_model_uris existen en el grafo
   # - Distribuci√≥n de dificultad/tipo cumple criterios
   ```

3. **Congelar entorno:**
   ```bash
   pip freeze > requirements_frozen.txt
   ```

### 3.2 Ejecuci√≥n de Benchmarks

```bash
# 1. Baseline BM25
python run_keyword_benchmark.py \
    --graph snapshot/graph_snapshot.ttl \
    --queries queries_50.jsonl \
    --results results_bm25.jsonl \
    --report report_bm25.json \
    --k 5

# 2. M√©todo 1 - Config A (principal)
python run_text2sparql_benchmark.py \
    --graph snapshot/graph_snapshot.ttl \
    --queries queries_50.jsonl \
    --results results_method1_configA.jsonl \
    --report report_method1_configA.json \
    --k 5 \
    --use-rag \
    --top-k-examples 3 \
    --temperature 0.1

# 3. M√©todo 1 - Config B (sin RAG)
python run_text2sparql_benchmark.py \
    --graph snapshot/graph_snapshot.ttl \
    --queries queries_50.jsonl \
    --results results_method1_configB.jsonl \
    --report report_method1_configB.json \
    --k 5 \
    --no-rag \
    --temperature 0.1

# 4. Config C y D (ablations)
# ... (similar)
```

### 3.3 An√°lisis de Resultados

```bash
# Comparar todos los m√©todos
python compare_results.py \
    --reports report_bm25.json report_method1_configA.json report_method1_configB.json \
    --output comparison_table.csv \
    --figures figures/

# An√°lisis de errores
python analyze_errors.py \
    --results results_method1_configA.jsonl \
    --queries queries_50.jsonl \
    --output error_analysis.json
```

---

## 4. An√°lisis de Errores Cualitativo

**Taxonom√≠a de Errores (para an√°lisis manual):**

1. **Errores de Sintaxis:**
   - Llaves desbalanceadas
   - Prefijos faltantes
   - Sintaxis SPARQL inv√°lida

2. **Errores Sem√°nticos:**
   - **Filtro faltante**: Consulta no captura restricci√≥n del prompt
   - **Propiedad incorrecta**: Usa `daimo:library` en vez de `daimo:framework`
   - **Valor incorrecto**: Usa literal sin tipo cuando debe ser typed
   - **Sobregeneralizaci√≥n**: Devuelve demasiados resultados
   - **Subgeneralizaci√≥n**: Devuelve muy pocos resultados

3. **Errores de Comprensi√≥n:**
   - No entiende intenci√≥n (ej: "top 5" ‚Üí no usa LIMIT)
   - Malinterpreta entidad (ej: "PyTorch" ‚Üí busca "pytorch" en descripci√≥n)

**M√©todo:**
- Seleccionar aleatoriamente 20 errores (queries con F1 < 0.5)
- Clasificar manualmente seg√∫n taxonom√≠a
- Reportar frecuencia de cada tipo
- Incluir ejemplos representativos en paper/tesis

---

## 5. Tests Estad√≠sticos

### 5.1 Comparaci√≥n M√©todo 1 vs Baseline BM25

**Hip√≥tesis:**
- H‚ÇÄ: P@5_method1 ‚â§ P@5_baseline (no hay mejora)
- H‚ÇÅ: P@5_method1 > P@5_baseline (hay mejora)

**Test:**
- **Paired t-test** (porque cada query se eval√∫a con ambos m√©todos)
- Nivel de significancia: Œ± = 0.05
- Si p-value < 0.05 ‚Üí Rechazar H‚ÇÄ (mejora significativa)

**Implementaci√≥n en Python:**
```python
from scipy import stats
import numpy as np

# p5_method1 = array de P@5 por query para M√©todo 1
# p5_baseline = array de P@5 por query para Baseline

# Paired t-test (one-tailed)
t_stat, p_value = stats.ttest_rel(p5_method1, p5_baseline, alternative='greater')

print(f"t-statistic: {t_stat:.3f}")
print(f"p-value: {p_value:.4f}")
if p_value < 0.05:
    print("‚úÖ Mejora estad√≠sticamente significativa (Œ±=0.05)")
else:
    print("‚ùå Sin evidencia de mejora significativa")
```

**Aplicar a todas las m√©tricas principales:**
- Precision@5
- Recall@5
- F1@5
- NDCG@5
- MRR

### 5.2 Intervalos de Confianza

Para cada m√©trica, reportar:
- Media
- Desviaci√≥n est√°ndar
- Intervalo de confianza al 95%

```python
import numpy as np
from scipy import stats

def confidence_interval(data, confidence=0.95):
    n = len(data)
    mean = np.mean(data)
    std_err = stats.sem(data)
    margin = std_err * stats.t.ppf((1 + confidence) / 2, n - 1)
    return mean, mean - margin, mean + margin

# Ejemplo:
mean, ci_low, ci_high = confidence_interval(p5_method1)
print(f"P@5: {mean:.3f} ¬± {mean - ci_low:.3f} (95% CI: [{ci_low:.3f}, {ci_high:.3f}])")
```

---

## 6. Estructura del Reporte Final

### 6.1 Para Paper/Tesis

**Secci√≥n 4: Evaluaci√≥n Experimental**

**4.1 Dise√±o Experimental**
- Descripci√≥n del snapshot (tama√±o, fuentes, fecha)
- Construcci√≥n del benchmark (n=50, distribuci√≥n)
- M√©tricas seleccionadas (con justificaci√≥n)
- Baseline comparativo

**4.2 Configuraci√≥n**
- Hiperpar√°metros del M√©todo 1
- Entorno de ejecuci√≥n (hardware, software)
- Reproducibilidad (c√≥digo/snapshot disponibles)

**4.3 Resultados**

**Tabla 1: Resultados Principales (M√©todo 1 Config A vs Baseline BM25)**

| M√©trica | Baseline | M√©todo 1 | Œî | p-value |
|---------|----------|----------|---|---------|
| P@5 | 0.45 ¬± 0.03 | **0.72 ¬± 0.04** | +60% | < 0.001 ‚úÖ |
| R@5 | 0.38 ¬± 0.04 | **0.65 ¬± 0.05** | +71% | < 0.001 ‚úÖ |
| F1@5 | 0.41 ¬± 0.03 | **0.68 ¬± 0.04** | +66% | < 0.001 ‚úÖ |
| NDCG@5 | 0.52 ¬± 0.03 | **0.76 ¬± 0.03** | +46% | < 0.001 ‚úÖ |
| MRR | 0.48 ¬± 0.04 | **0.73 ¬± 0.04** | +52% | < 0.001 ‚úÖ |
| Latency (ms) | 35 ¬± 8 | 2,450 ¬± 380 | +6900% | < 0.001 |

*(Valores hipot√©ticos para ilustraci√≥n)*

**Tabla 2: Ablation Studies**

| Config | RAG | Temp | P@5 | Œî vs A |
|--------|-----|------|-----|--------|
| A (principal) | ‚úÖ | 0.1 | 0.72 | - |
| B (sin RAG) | ‚ùå | 0.1 | 0.58 | -19% |
| C (temp alta) | ‚úÖ | 0.5 | 0.68 | -6% |
| D (m√°s ejemplos) | ‚úÖ | 0.1 | 0.74 | +3% |

**Figura 1:** Precision-Recall curve por dificultad (b√°sico/medio/avanzado)
**Figura 2:** Distribuci√≥n de errores (taxonom√≠a cualitativa)
**Tabla 3:** Ejemplos de errores representativos

**4.4 Discusi√≥n**
- ¬øD√≥nde funciona bien el M√©todo 1?
- ¬øD√≥nde falla m√°s?
- Limitaciones observadas
- Trade-off latency vs precisi√≥n

---

## 7. Checklist de Validaci√≥n Acad√©mica

Antes de enviar el paper/tesis, verificar:

### Reproducibilidad
- [ ] Snapshot del grafo disponible con SHA256
- [ ] C√≥digo en repositorio p√∫blico con instrucciones
- [ ] requirements.txt con versiones exactas
- [ ] Benchmark queries versionado (queries.jsonl)
- [ ] README explica c√≥mo reproducir todos los resultados

### Rigor Estad√≠stico
- [ ] n ‚â• 50 queries (o justificaci√≥n si n < 50)
- [ ] Tests de significancia reportados
- [ ] Intervalos de confianza incluidos
- [ ] Varianza/std reportada
- [ ] Ablation studies ejecutados

### Validez
- [ ] Ground truth verificado manualmente
- [ ] Baseline apropiado (no "strawman")
- [ ] M√©tricas est√°ndar del campo (IR/QA)
- [ ] An√°lisis cualitativo de errores
- [ ] Distribuci√≥n representativa de queries

### Transparencia
- [ ] Limitaciones expl√≠citas
- [ ] Casos de fallo documentados
- [ ] Hiperpar√°metros justificados
- [ ] Trade-offs discutidos

---

## 8. Recursos y Referencias

### M√©tricas de IR/QA
- Manning et al. (2008): *Introduction to Information Retrieval*
- J√§rvelin & Kek√§l√§inen (2002): NDCG original paper

### Benchmarks Similares
- QALD: Question Answering over Linked Data
- LC-QuAD: Large-scale Complex Question Answering Dataset
- Spider: Text-to-SQL benchmark

### Reproducibilidad
- Pineau (2021): NeurIPS reproducibility checklist
- ACM Artifact Review and Badging

---

## Resumen Ejecutivo

**Pasos Cr√≠ticos para Validaci√≥n Acad√©mica del M√©todo 1:**

1. ‚úÖ **Snapshot reproducible** (SHA256 + metadata)
2. ‚úÖ **50 queries** con ground truth verificado (distribuci√≥n 40/40/20)
3. ‚úÖ **Baseline BM25** ejecutado en las mismas queries
4. ‚úÖ **M√©tricas est√°ndar** (P/R/F1/NDCG@5, MRR, MAP)
5. ‚úÖ **Ablation studies** (con RAG vs sin RAG, temperatura, k_examples)
6. ‚úÖ **Tests estad√≠sticos** (paired t-test, p-values, IC 95%)
7. ‚úÖ **An√°lisis cualitativo** de errores (taxonom√≠a + ejemplos)
8. ‚úÖ **Reporte transparente** (limitaciones + trade-offs)

**Tiempo Estimado:**
- Preparaci√≥n (snapshot + benchmark expansion): 1-2 d√≠as
- Ejecuci√≥n de experimentos: 2-4 horas (autom√°tico)
- An√°lisis de errores: 2-3 d√≠as
- Redacci√≥n: 1 semana

**Output Final:**
- Paper/Tesis con validaci√≥n experimental rigurosa
- C√≥digo y datos reproducibles
- Aceptaci√≥n en conferencia/revista de calidad
# Benchmark Schema (Baseline: Keyword Search over Graph Metadata)

This schema defines the benchmark dataset and evaluation outputs for the
**simple search baseline** (keyword matching over graph metadata) and the
**proposed method** (Text-to-SPARQL). Both methods must be evaluated with the
same metrics.

---

## 1. Dataset Format (JSONL)

File: `experiments/benchmarks/queries.jsonl`

One JSON object per line. Minimal required fields:

```json
{
  "id": "q001",
  "query_nl": "pytorch models for image classification with MIT license",
  "query_keywords": ["pytorch", "image", "classification", "mit"],
  "difficulty": "medium",
  "gold_sparql": "PREFIX daimo: ... SELECT ?model WHERE { ... }",
  "gold_model_uris": [
    "http://purl.org/pionera/daimo#model/hf/resnet50",
    "http://purl.org/pionera/daimo#model/pytorch/xyz"
  ],
  "notes": "Filters on framework, task, license"
}
```

### Field Definitions

- `id` (string, required): Unique query id. Use `qNNN`.
- `query_nl` (string, required): Natural language query.
- `query_keywords` (list[string], required): Canonical keywords used by the baseline.
  - Baseline uses these tokens to match against metadata text fields.
  - Keep in lowercase, stemmed/lemmatized if you apply normalization.
- `difficulty` (string, required): `basic`, `medium`, `advanced`.
- `gold_sparql` (string, recommended): Ground truth SPARQL query used to produce gold set.
- `gold_model_uris` (list[string], required): Gold set of model URIs (ground truth).
- `notes` (string, optional): Annotation notes for analysis.

### Optional Fields (use when needed)

- `filters` (object): Structured constraints if known.
  - Example: `{ "framework": ["pytorch"], "task": ["image-classification"], "license": ["MIT"] }`
- `expected_count` (int): Expected size of gold set.
- `language` (string): `es` or `en` (if multilingual queries are included).

---

## 2. Baseline Definition (Keyword Search)

Baseline algorithm:

1. Normalize `query_keywords` (lowercase, remove stopwords if configured).
2. Index each model using concatenated metadata fields from the graph:
   - `daimo:title`, `daimo:description`, `daimo:task`, `daimo:framework`,
     `daimo:license`, `daimo:repository`, `daimo:tags`, `daimo:author`,
     `daimo:dataset`, `daimo:paperTitle`
3. Compute score using BM25 (fixed).
4. Return top K results ranked by score.

Important: This baseline **does not** use SPARQL or semantic parsing.

---

## 3. Evaluation Metrics (Common to Both Methods)

Compute for each query and aggregate (mean, median, std). **K = 5**.

Ranking metrics:
- `Precision@5`
- `Recall@5`
- `F1@5`
- `nDCG@5`
- `MRR` (Mean Reciprocal Rank)
- `MAP@5`

System metrics:
- `Latency` (avg, p95)
- `Hit@5` (a.k.a. Success@5): 1 if any relevant result in top 5
- `Coverage`: % queries with non-empty result set
- `ExecutionSuccess`: % queries that return results without error

Optional (recommended if you want more rigor):
- `ExactMatch`: returned set equals gold set (set equality)
- `Jaccard`: overlap between returned set and gold set
- `ResultCountError`: absolute difference between returned count and gold count

---

## 4. Results Format (JSONL)

File: `experiments/benchmarks/results.jsonl`

One JSON object per method per query:

```json
{
  "id": "q001",
  "method": "keyword_baseline",
  "top_k": 10,
  "results": [
    "http://purl.org/pionera/daimo#model/hf/resnet50",
    "http://purl.org/pionera/daimo#model/pytorch/xyz"
  ],
  "latency_ms": 38.2,
  "error": null
}
```

---

## 5. Aggregated Report (JSON)

File: `experiments/benchmarks/report.json`

Example:

```json
{
  "method": "keyword_baseline",
  "k_values": [5],
  "precision_at_k": {"5": 0.31},
  "recall_at_k": {"5": 0.28},
  "f1_at_k": {"5": 0.29},
  "ndcg_at_k": {"5": 0.35},
  "mrr": 0.46,
  "map_at_5": 0.24,
  "hit_at_k": {"5": 0.63},
  "coverage": 0.95,
  "execution_success": 1.0,
  "latency_ms_avg": 41.7,
  "latency_ms_p95": 84.3
}
```

---

## 6. Minimal Starter Checklist

- Create `queries.jsonl` with 50-100 entries.
- Fill `gold_sparql` and `gold_model_uris`.
- Implement baseline keyword search (BM25 or TF-IDF).
- Evaluate both methods with the same metrics.
# üéØ Resumen Ejecutivo - Sistema de Evaluaci√≥n Acad√©mica

**Fecha:** 2026-02-11  
**Estado:** ‚úÖ Sistema completo implementado  
**Objetivo:** Validaci√≥n acad√©mica rigurosa del M√©todo 1 (Text-to-SPARQL) para paper/tesis

---

## ‚úÖ ¬øQu√© se ha implementado?

### 1. üìê Dise√±o Experimental Riguroso

**Archivo:** [`EVALUATION_DESIGN.md`](./EVALUATION_DESIGN.md)

- ‚úÖ Marco te√≥rico completo para validaci√≥n acad√©mica
- ‚úÖ Requisitos para publicaci√≥n cient√≠fica
- ‚úÖ Protocolo de reproducibilidad
- ‚úÖ M√©tricas est√°ndar de IR/QA
- ‚úÖ Dise√±o de ablation studies
- ‚úÖ Tests estad√≠sticos (paired t-test, CI, Cohen's d)
- ‚úÖ Taxonom√≠a de errores para an√°lisis cualitativo
- ‚úÖ Checklist de validaci√≥n acad√©mica

**Contenido clave:**
- Snapshot reproducible con SHA256
- Benchmark de 50 queries (distribuci√≥n 40/40/20)
- Baseline BM25 para comparaci√≥n justa
- 4 configuraciones experimentales (ablations)
- Tests de significancia estad√≠stica
- An√°lisis cualitativo de errores

---

### 2. üìì Notebook Interactivo Completo

**Archivo:** [`evaluation_pipeline.ipynb`](./evaluation_pipeline.ipynb)

Pipeline end-to-end que ejecuta TODO el proceso de evaluaci√≥n:

#### Secciones del Notebook:

1. **‚öôÔ∏è Configuraci√≥n Inicial**
   - Setup de paths y dependencias
   - Creaci√≥n de directorios

2. **1Ô∏è‚É£ Creaci√≥n de Snapshot Reproducible**
   - Copia del grafo RDF
   - C√°lculo de SHA256
   - Extracci√≥n de estad√≠sticas
   - Generaci√≥n de metadatos JSON

3. **2Ô∏è‚É£ An√°lisis Exploratorio**
   - Estad√≠sticas del benchmark actual (12 queries)
   - Distribuci√≥n por dificultad y tipo
   - Visualizaciones

4. **3Ô∏è‚É£ Expansi√≥n del Benchmark**
   - Generaci√≥n autom√°tica de ~40 queries adicionales
   - Template-based: task, library, source, license
   - Queries de ranking (TOP-K)
   - Queries de agregaci√≥n (COUNT, GROUP BY)
   - Ejecuta SPARQL gold para obtener ground truth
   - Guarda en `queries_50.jsonl`

5. **4Ô∏è‚É£ Validaci√≥n de Ground Truth**
   - Verifica que todos los SPARQL gold ejecuten
   - Compara resultados esperados vs actuales
   - Detecta inconsistencias

6. **5Ô∏è‚É£ Ejecuci√≥n de Benchmarks**
   - Baseline BM25 (keyword search)
   - M√©todo 1 Config-A (principal: RAG + temp=0.1 + k=3)
   - M√©todo 1 Config-B (ablation: sin RAG)
   - M√©todo 1 Config-C (ablation: temp=0.5)
   - Ejecuci√≥n autom√°tica con subprocess

7. **6Ô∏è‚É£ An√°lisis de Resultados**
   - Carga de reportes JSON
   - Tabla comparativa de m√©tricas
   - **Tests estad√≠sticos:**
     - Paired t-test (one-tailed)
     - p-values
     - Cohen's d (effect size)
     - 95% Confidence Intervals
   - Identificaci√≥n de mejoras significativas

8. **7Ô∏è‚É£ An√°lisis de Errores**
   - Identificaci√≥n de queries con F1 < 0.5
   - Clasificaci√≥n por dificultad
   - Ejemplos representativos
   - Estad√≠sticas por categor√≠a

9. **8Ô∏è‚É£ Visualizaciones**
   - Comparaci√≥n de m√©tricas (barras)
   - Latencias (avg vs p95)
   - Rendimiento por dificultad (boxplots)
   - Exporta PNG de alta calidad (300 DPI)

10. **9Ô∏è‚É£ Reporte Final**
    - Genera `FINAL_REPORT.md` en Markdown
    - Incluye todas las tablas y estad√≠sticas
    - Listo para copiar a paper/tesis
    - Conclusiones y pr√≥ximos pasos

---

### 3. üõ†Ô∏è Scripts Auxiliares

#### `create_snapshot.py`

Crea snapshot reproducible del grafo RDF.

**Uso:**
```bash
python create_snapshot.py \
    --source ../../data/ai_models_multi_repo.ttl \
    --output ./snapshot
```

**Genera:**
- `snapshot/graph_snapshot.ttl` (grafo congelado)
- `snapshot/snapshot_metadata.json` (SHA256 + estad√≠sticas)
- `snapshot/README.md` (documentaci√≥n)

**Caracter√≠sticas:**
- SHA256 para verificaci√≥n de integridad
- Estad√≠sticas completas (# modelos, tripletas, sources, tasks)
- Formato JSON con metadatos estructurados

---

#### `validate_benchmark.py`

Valida que el benchmark cumple est√°ndares de calidad.

**Uso:**
```bash
python validate_benchmark.py \
    --queries queries_50.jsonl \
    --graph snapshot/graph_snapshot.ttl \
    --output validation_report.json
```

**Validaciones:**
- ‚úÖ Todos los SPARQL gold ejecutables
- ‚úÖ URIs gold coinciden con resultados actuales
- ‚úÖ Campos requeridos presentes
- ‚úÖ Distribuci√≥n balanceada (dificultad/tipo)
- ‚ö†Ô∏è Advertencias (queries vac√≠as, demasiados resultados)

---

### 4. üìä M√©tricas Implementadas

**Archivo:** [`metrics.py`](./metrics.py)

Funciones para todas las m√©tricas est√°ndar de IR/QA:

- `precision_at_k()` - Precisi√≥n en top-K
- `recall_at_k()` - Cobertura en top-K
- `f1_at_k()` - Media arm√≥nica
- `ndcg_at_k()` - Normalized DCG
- `mrr()` - Mean Reciprocal Rank
- `map_at_k()` - Mean Average Precision
- `hit_at_k()` - Success@K
- `exact_match()` - Igualdad de conjuntos
- `jaccard()` - Similitud Jaccard
- `result_count_error()` - Error en conteo
- `aggregate()` - Media, mediana, std
- `percentile()` - Percentiles (p95, p99)

---

### 5. üéØ Baseline BM25

**Archivos:**
- [`keyword_bm25.py`](./keyword_bm25.py) - Implementaci√≥n
- [`run_keyword_benchmark.py`](./run_keyword_benchmark.py) - Script ejecutable

**Caracter√≠sticas:**
- BM25 con par√°metros est√°ndar (k1=1.5, b=0.75)
- √çndice sobre: t√≠tulo, descripci√≥n, task, library, license, tags, source
- Tokenizaci√≥n + normalizaci√≥n
- Top-K ranking por score

**Justificaci√≥n:**
- BM25 es el est√°ndar de facto en IR
- No requiere entrenamiento ‚Üí reproducible
- Fair comparison (mismas queries, mismo K)

---

### 6. üìÑ Documentaci√≥n Completa

**Archivos:**
- [`README.md`](./README.md) - Gu√≠a principal de uso
- [`EVALUATION_DESIGN.md`](./EVALUATION_DESIGN.md) - Dise√±o experimental detallado
- [`benchmark_schema.md`](./benchmark_schema.md) - Esquema del benchmark

**Contenido:**
- Quickstart guide
- Estructura de directorios
- Instrucciones paso a paso
- Troubleshooting
- Referencias acad√©micas
- Checklist de validaci√≥n

---

## üöÄ Pr√≥ximos Pasos (Para el Usuario)

### PASO 1: Ejecutar el Notebook

**Recomendaci√≥n:** Empezar con el notebook interactivo.

```bash
# Abrir en VS Code
code evaluation_pipeline.ipynb

# O con Jupyter
jupyter notebook evaluation_pipeline.ipynb
```

**Ejecutar celdas en orden:**
1. ‚úÖ Configuraci√≥n
2. ‚úÖ Crear snapshot
3. ‚úÖ Analizar benchmark actual
4. ‚úÖ Expandir a 50 queries (autom√°tico con templates)
5. ‚úÖ Validar ground truth
6. ‚è±Ô∏è Ejecutar benchmarks (30-60 min)
7. ‚úÖ An√°lisis estad√≠stico
8. ‚úÖ Visualizaciones
9. ‚úÖ Reporte final

---

### PASO 2: Revisar Queries Generadas

El notebook generar√° ~40 queries adicionales autom√°ticamente. **Es importante revisar manualmente:**

```bash
# Ver queries generadas
cat queries_50.jsonl

# Validar
python validate_benchmark.py \
    --queries queries_50.jsonl \
    --graph snapshot/graph_snapshot.ttl
```

**Acciones recomendadas:**
- [ ] Verificar que las queries NL tienen sentido
- [ ] Ajustar keywords si es necesario
- [ ] A√±adir queries manualmente para casos especiales
- [ ] Balancear distribuci√≥n (objetivo: 20 b√°sicas, 20 medias, 10 avanzadas)

---

### PASO 3: Ejecutar Benchmarks Completos

Si prefieres ejecutar fuera del notebook:

```bash
# 1. Baseline BM25
python run_keyword_benchmark.py \
    --graph snapshot/graph_snapshot.ttl \
    --queries queries_50.jsonl \
    --k 5

# 2. M√©todo 1 Principal
python run_text2sparql_benchmark.py \
    --graph snapshot/graph_snapshot.ttl \
    --queries queries_50.jsonl \
    --use-rag \
    --top-k-examples 3 \
    --temperature 0.1

# 3. Ablation: Sin RAG
python run_text2sparql_benchmark.py \
    --graph snapshot/graph_snapshot.ttl \
    --queries queries_50.jsonl \
    --no-rag \
    --temperature 0.1

# 4. Ablation: Temp alta
python run_text2sparql_benchmark.py \
    --graph snapshot/graph_snapshot.ttl \
    --queries queries_50.jsonl \
    --use-rag \
    --temperature 0.5
```

**Tiempo estimado:** 30-60 minutos (depende del LLM)

---

### PASO 4: An√°lisis de Resultados

Los resultados estar√°n en:
- `results/report_*.json` - M√©tricas agregadas
- `results/comparison_table.csv` - Tabla comparativa
- `results/statistical_tests.csv` - Significancia estad√≠stica
- `results/FINAL_REPORT.md` - Reporte completo

**Revisar:**
- [ ] M√©tricas principales (P@5, R@5, F1@5, NDCG@5, MRR)
- [ ] p-values < 0.05 ‚Üí mejora significativa
- [ ] Queries problem√°ticas (error_analysis.csv)
- [ ] Visualizaciones (figures/*.png)

---

### PASO 5: An√°lisis Cualitativo de Errores

**Seleccionar muestra de errores:**
```bash
# Ver queries con peor rendimiento
cat results/error_analysis.csv | head -20
```

**Clasificar manualmente seg√∫n taxonom√≠a:**
1. **Errores sint√°cticos:** SPARQL inv√°lido
2. **Errores sem√°nticos:**
   - Filtro faltante
   - Propiedad incorrecta
   - Valor incorrecto
   - Sobregeneralizaci√≥n/subgeneralizaci√≥n
3. **Errores de comprensi√≥n:** No entiende intenci√≥n

**Documentar:** Incluir ejemplos en paper/tesis.

---

### PASO 6: Redacci√≥n de Paper/Tesis

Usar el material generado:

**Secci√≥n 4: Evaluaci√≥n Experimental**

**4.1 Dise√±o Experimental**
- Copiar desde `EVALUATION_DESIGN.md` ‚Üí Snapshot, benchmark, m√©tricas

**4.2 Configuraci√≥n**
- Tabla de hiperpar√°metros
- Entorno (hardware, software)

**4.3 Resultados**
- **Tabla 1:** `comparison_table.csv` ‚Üí M√©todo 1 vs Baseline
- **Tabla 2:** Ablation studies (Config A vs B vs C vs D)
- **Tabla 3:** `statistical_tests.csv` ‚Üí Tests de significancia

**4.4 An√°lisis**
- **Figura 1:** `metrics_comparison.png`
- **Figura 2:** `performance_by_difficulty.png`
- **Tabla 4:** Ejemplos de errores (error_analysis.csv)

**4.5 Discusi√≥n**
- Copiar conclusiones desde `FINAL_REPORT.md`
- Limites observadas
- Trade-offs (precision vs latency)

---

## üìä Resultados Esperados

### Hip√≥tesis

**H‚ÇÅ:** El M√©todo 1 (Text-to-SPARQL) superar√° significativamente al baseline BM25 en m√©tricas de recuperaci√≥n.

**Predicciones:**
- P@5: M√©todo1 > 0.65, Baseline ‚âà 0.45
- F1@5: M√©todo1 > 0.60, Baseline ‚âà 0.40
- p-value < 0.05 (significativo)

**Trade-off:**
- Latencia: M√©todo1 >> Baseline (esperado 50-100x m√°s lento)

### Impacto del RAG

**Hip√≥tesis:** RAG mejorar√° rendimiento vs sin RAG.

**An√°lisis:** Config-A vs Config-B

### Impacto de Temperatura

**Hip√≥tesis:** Temp baja (0.1) mejor que alta (0.5) para precisi√≥n.

**An√°lisis:** Config-A vs Config-C

---

## ‚úÖ Checklist Final

Antes de enviar paper/tesis:

### Reproducibilidad
- [ ] Snapshot con SHA256 documentado
- [ ] C√≥digo en GitHub con instrucciones
- [ ] requirements.txt actualizado
- [ ] Benchmark queries versionadas

### Validaci√≥n Estad√≠stica
- [ ] n ‚â• 50 queries
- [ ] Paired t-tests ejecutados
- [ ] Intervalos de confianza reportados
- [ ] Ablation studies completados

### Calidad
- [ ] Ground truth verificado manualmente (muestra)
- [ ] Baseline apropiado (BM25)
- [ ] An√°lisis cualitativo de errores
- [ ] Visualizaciones de alta calidad (300 DPI)

### Transparencia
- [ ] Limitaciones expl√≠citas
- [ ] Casos de fallo documentados
- [ ] Hiperpar√°metros justificados

---

## üìû Soporte

**Problemas comunes:**

1. **Grafo no encontrado** ‚Üí Construir con `build_graph.py`
2. **LLM no responde** ‚Üí Verificar Ollama con `ollama list`
3. **Queries inv√°lidas** ‚Üí Ejecutar `validate_benchmark.py`
4. **Latencia excesiva** ‚Üí Reducir # queries o usar modelo m√°s peque√±o

**Documentaci√≥n:**
- README principal: [`README.md`](./README.md)
- Dise√±o experimental: [`EVALUATION_DESIGN.md`](./EVALUATION_DESIGN.md)

---

## üéì Impacto Acad√©mico

Este sistema de evaluaci√≥n cumple con est√°ndares de:

- ‚úÖ **ACM Artifact Review and Badging** (reproducibilidad)
- ‚úÖ **NeurIPS Reproducibility Checklist**
- ‚úÖ **Est√°ndares de IR/QA** (m√©tricas TREC-style)
- ‚úÖ **Rigor estad√≠stico** (tests param√©tricos, CI, effect size)

**Resultado esperado:**
- Paper aceptado en conferencia de calidad (ACM, IEEE, etc.)
- Cap√≠tulo de tesis con validaci√≥n experimental s√≥lida
- Datos y c√≥digo reutilizables por otros investigadores

---

## üèÜ Conclusi√≥n

**Has recibido un sistema completo de evaluaci√≥n acad√©mica que:**

1. ‚úÖ Implementa reproducibilidad perfecta (snapshot + SHA256)
2. ‚úÖ Genera benchmark de calidad (50 queries balanceadas)
3. ‚úÖ Ejecuta comparaciones justas (baseline BM25)
4. ‚úÖ Realiza an√°lisis estad√≠stico riguroso (t-tests, CI, effect size)
5. ‚úÖ Produce visualizaciones profesionales (300 DPI)
6. ‚úÖ Genera reportes listos para publicaci√≥n

**Todo el proceso est√° documentado y puede ejecutarse:**
- ‚úÖ Interactivamente (notebook)
- ‚úÖ Por scripts individuales
- ‚úÖ De forma reproducible (snapshot fijo)

**Pr√≥ximo paso inmediato:**
üëâ Ejecutar `evaluation_pipeline.ipynb` y seguir las celdas en orden.

---

**¬°Mucho √©xito con tu investigaci√≥n! üöÄ**

*Este sistema fue dise√±ado para cumplir est√°ndares acad√©micos internacionales y facilitar la validaci√≥n rigurosa del M√©todo 1.*
# Notebook 03: Text-to-SPARQL Validation

## üéØ Objetivo

Validar el sistema completo de conversi√≥n de lenguaje natural a SPARQL usando:
- **TextToSPARQLConverter** con LangChain
- **RAG** con ChromaDB (17 ejemplos)
- **Grafo RDF** con 175 modelos de IA

---

## üìã Contenido del Notebook

### 1. Setup (Celdas 1-3)
- Imports y configuraci√≥n de paths
- Cargar m√≥dulo `llm` con conversor
- Verificaci√≥n de dependencias

### 2. Cargar Grafo RDF (Celda 4)
- Cargar `data/multi_repository_kg.ttl` (del notebook 02)
- Estad√≠sticas: 5,829 triples, 175 modelos
- Namespace DAIMO configurado

### 3. Verificar Base de Conocimiento (Celda 5)
- Cargar 17 ejemplos SPARQL
- Distribuci√≥n por complejidad (basic/intermediate/advanced)
- Mostrar ejemplos de cada nivel

### 4. Inicializar Conversor (Celda 6)
- Verificar `ANTHROPIC_API_KEY`
- Crear `TextToSPARQLConverter(use_rag=True)`
- Configurar RAG con ChromaDB

### 5. Test Queries (Celdas 7-8)
- **10 queries de prueba**:
  - 4 b√°sicas (filtrado, ordenamiento)
  - 3 intermedias (multi-criterio)
  - 3 avanzadas (agregaciones, negaci√≥n)

### 6. Ejecutar Conversiones (Celda 9)
- Procesar todas las queries
- Almacenar resultados con metadata:
  - SPARQL generado
  - Validaci√≥n (is_valid)
  - Confianza (high/medium/low)
  - Ejemplos RAG recuperados

### 7. An√°lisis de Resultados (Celda 10)
- **Success rate general** (target: ‚â•70%)
- **Success rate por complejidad**
- **Distribuci√≥n de confianza**
- Listar queries inv√°lidas

### 8. An√°lisis RAG (Celda 11)
- Top ejemplos m√°s recuperados
- Distribuci√≥n de ejemplos por complejidad de query
- Verificar relevancia de RAG retrieval

### 9. Ejecuci√≥n contra RDF (Celda 12)
- Ejecutar queries v√°lidas en el grafo
- Contar resultados obtenidos
- Detectar errores de ejecuci√≥n

### 10. Ejemplo Detallado (Celda 13)
- Demo completa de conversi√≥n + ejecuci√≥n
- Mostrar SPARQL generado
- Mostrar primeros 5 resultados del grafo

### 11. Visualizaciones (Celda 14)
- 4 gr√°ficos:
  1. Success rate general (bar chart)
  2. Success rate por complejidad (bar chart)
  3. Distribuci√≥n de confianza (bar chart)
  4. Top 5 ejemplos RAG (horizontal bar)

### 12. Resumen Final (Celda 15)
- M√©tricas consolidadas
- Evaluaci√≥n vs objetivo (70%)
- Pr√≥ximos pasos

### 13. Export (Celda 16)
- Guardar resultados en CSV
- Archivo: `data/text_to_sparql_validation_results.csv`

---

## üöÄ C√≥mo Ejecutar

### Pre-requisitos

1. **Instalar dependencias LangChain + RAG**:
   ```bash
   cd /home/edmundo/ai-model-discovery
   ./llm/install_langchain.sh
   ```

2. **Configurar API key**:
   ```bash
   export ANTHROPIC_API_KEY='tu-api-key-aqui'
   ```

3. **Verificar grafo RDF existe**:
   ```bash
   ls -lh data/multi_repository_kg.ttl
   # Si no existe, ejecutar notebook 02 primero
   ```

### Ejecutar Notebook

```bash
# Opci√≥n 1: Jupyter Notebook
jupyter notebook notebooks/03_text_to_sparql_validation.ipynb

# Opci√≥n 2: JupyterLab
jupyter lab notebooks/03_text_to_sparql_validation.ipynb

# Opci√≥n 3: VS Code
# Abrir directamente en VS Code con extensi√≥n de Jupyter
```

---

## üìä M√©tricas Esperadas

| M√©trica | Target | Descripci√≥n |
|---------|--------|-------------|
| **Success Rate** | ‚â•70% | % de queries v√°lidas |
| **Basic Success** | ‚â•90% | Queries b√°sicas v√°lidas |
| **Intermediate Success** | ‚â•80% | Queries intermedias v√°lidas |
| **Advanced Success** | ‚â•70% | Queries avanzadas v√°lidas |
| **High Confidence** | ‚â•60% | Queries con confianza alta |
| **Execution Rate** | ‚â•90% | Queries que ejecutan sin error |

---

## üîç Test Queries Incluidas

### B√°sicas (4)
1. "show me the most popular models"
2. "computer vision models"
3. "models from HuggingFace"
4. "NLP models with high downloads"

### Intermedias (3)
5. "PyTorch models with high downloads"
6. "NLP models from HuggingFace or Kaggle"
7. "models with both high downloads and high rating"

### Avanzadas (3)
8. "compare PyTorch vs TensorFlow by average downloads"
9. "count models by task category"
10. "models NOT from HuggingFace"

---

## üìà An√°lisis Incluidos

1. **Validaci√≥n Sint√°ctica**
   - PREFIX correcto
   - Estructura SELECT/WHERE
   - No operaciones peligrosas (DELETE, DROP)

2. **RAG Performance**
   - Ejemplos m√°s recuperados
   - Relevancia de retrieval
   - Distribuci√≥n por complejidad

3. **Ejecuci√≥n Real**
   - Queries ejecutadas exitosamente
   - N√∫mero de resultados obtenidos
   - Errores de runtime

4. **Confidence Scoring**
   - High: Query completa sin warnings
   - Medium: Query v√°lida con warnings
   - Low: Query inv√°lida o incompleta

---

## üìù Outputs Generados

1. **Visualizaciones**: 4 gr√°ficos de an√°lisis
2. **CSV Export**: `data/text_to_sparql_validation_results.csv`
3. **Logs detallados**: En celdas con print statements

---

## üêõ Troubleshooting

### Error: "ANTHROPIC_API_KEY no configurada"
```bash
export ANTHROPIC_API_KEY='sk-ant-...'
```

### Error: "Archivo RDF no encontrado"
```bash
# Ejecutar notebook 02 primero
jupyter notebook notebooks/02_multi_repository_validation.ipynb
```

### Error: "Module 'llm' not found"
```bash
# Verificar que est√°s en el directorio correcto
cd /home/edmundo/ai-model-discovery
python3 -c "from llm import TextToSPARQLConverter; print('OK')"
```

### Error: "ChromaDB no disponible"
```bash
pip install chromadb
```

---

## üîó Relacionado

- **Notebook 02**: `02_multi_repository_validation.ipynb` (genera el grafo RDF)
- **M√≥dulo LLM**: `llm/text_to_sparql.py` (conversor principal)
- **Base de Conocimiento**: `llm/rag_sparql_examples.py` (17 ejemplos)
- **Documentaci√≥n**: `llm/README_LANGCHAIN_RAG.md` (arquitectura)

---

## ‚úÖ Resultados Esperados

Al finalizar, deber√≠as tener:

- ‚úÖ **Success rate ‚â•70%** (7/10 queries v√°lidas)
- ‚úÖ **4 visualizaciones** de an√°lisis
- ‚úÖ **CSV exportado** con resultados detallados
- ‚úÖ **An√°lisis RAG** mostrando ejemplos relevantes
- ‚úÖ **Queries ejecutadas** contra grafo RDF real

Si el success rate es ‚â•70%, el sistema est√° listo para:
1. Integraci√≥n con SearchEngine
2. Creaci√≥n de notebook 04 (b√∫squeda end-to-end)
3. Implementaci√≥n de evaluaci√≥n (precision/recall)

---

**Estado**: ‚úÖ Notebook completo y listo para ejecutar  
**√öltima actualizaci√≥n**: Enero 2024
# Notebook 03: Text-to-SPARQL Validation

## üéØ Objetivo

Validar el sistema completo de conversi√≥n de lenguaje natural a SPARQL usando:
- **TextToSPARQLConverter** con LangChain
- **RAG** con ChromaDB (17 ejemplos)
- **Grafo RDF** con 175 modelos de IA

---

## üìã Contenido del Notebook

### 1. Setup (Celdas 1-3)
- Imports y configuraci√≥n de paths
- Cargar m√≥dulo `llm` con conversor
- Verificaci√≥n de dependencias

### 2. Cargar Grafo RDF (Celda 4)
- Cargar `data/multi_repository_kg.ttl` (del notebook 02)
- Estad√≠sticas: 5,829 triples, 175 modelos
- Namespace DAIMO configurado

### 3. Verificar Base de Conocimiento (Celda 5)
- Cargar 17 ejemplos SPARQL
- Distribuci√≥n por complejidad (basic/intermediate/advanced)
- Mostrar ejemplos de cada nivel

### 4. Inicializar Conversor (Celda 6)
- Verificar `ANTHROPIC_API_KEY`
- Crear `TextToSPARQLConverter(use_rag=True)`
- Configurar RAG con ChromaDB

### 5. Test Queries (Celdas 7-8)
- **10 queries de prueba**:
  - 4 b√°sicas (filtrado, ordenamiento)
  - 3 intermedias (multi-criterio)
  - 3 avanzadas (agregaciones, negaci√≥n)

### 6. Ejecutar Conversiones (Celda 9)
- Procesar todas las queries
- Almacenar resultados con metadata:
  - SPARQL generado
  - Validaci√≥n (is_valid)
  - Confianza (high/medium/low)
  - Ejemplos RAG recuperados

### 7. An√°lisis de Resultados (Celda 10)
- **Success rate general** (target: ‚â•70%)
- **Success rate por complejidad**
- **Distribuci√≥n de confianza**
- Listar queries inv√°lidas

### 8. An√°lisis RAG (Celda 11)
- Top ejemplos m√°s recuperados
- Distribuci√≥n de ejemplos por complejidad de query
- Verificar relevancia de RAG retrieval

### 9. Ejecuci√≥n contra RDF (Celda 12)
- Ejecutar queries v√°lidas en el grafo
- Contar resultados obtenidos
- Detectar errores de ejecuci√≥n

### 10. Ejemplo Detallado (Celda 13)
- Demo completa de conversi√≥n + ejecuci√≥n
- Mostrar SPARQL generado
- Mostrar primeros 5 resultados del grafo

### 11. Visualizaciones (Celda 14)
- 4 gr√°ficos:
  1. Success rate general (bar chart)
  2. Success rate por complejidad (bar chart)
  3. Distribuci√≥n de confianza (bar chart)
  4. Top 5 ejemplos RAG (horizontal bar)

### 12. Resumen Final (Celda 15)
- M√©tricas consolidadas
- Evaluaci√≥n vs objetivo (70%)
- Pr√≥ximos pasos

### 13. Export (Celda 16)
- Guardar resultados en CSV
- Archivo: `data/text_to_sparql_validation_results.csv`

---

## üöÄ C√≥mo Ejecutar

### Pre-requisitos

1. **Instalar dependencias LangChain + RAG**:
   ```bash
   cd /home/edmundo/ai-model-discovery
   ./llm/install_langchain.sh
   ```

2. **Configurar API key**:
   ```bash
   export ANTHROPIC_API_KEY='tu-api-key-aqui'
   ```

3. **Verificar grafo RDF existe**:
   ```bash
   ls -lh data/multi_repository_kg.ttl
   # Si no existe, ejecutar notebook 02 primero
   ```

### Ejecutar Notebook

```bash
# Opci√≥n 1: Jupyter Notebook
jupyter notebook notebooks/03_text_to_sparql_validation.ipynb

# Opci√≥n 2: JupyterLab
jupyter lab notebooks/03_text_to_sparql_validation.ipynb

# Opci√≥n 3: VS Code
# Abrir directamente en VS Code con extensi√≥n de Jupyter
```

---

## üìä M√©tricas Esperadas

| M√©trica | Target | Descripci√≥n |
|---------|--------|-------------|
| **Success Rate** | ‚â•70% | % de queries v√°lidas |
| **Basic Success** | ‚â•90% | Queries b√°sicas v√°lidas |
| **Intermediate Success** | ‚â•80% | Queries intermedias v√°lidas |
| **Advanced Success** | ‚â•70% | Queries avanzadas v√°lidas |
| **High Confidence** | ‚â•60% | Queries con confianza alta |
| **Execution Rate** | ‚â•90% | Queries que ejecutan sin error |

---

## üîç Test Queries Incluidas

### B√°sicas (4)
1. "show me the most popular models"
2. "computer vision models"
3. "models from HuggingFace"
4. "NLP models with high downloads"

### Intermedias (3)
5. "PyTorch models with high downloads"
6. "NLP models from HuggingFace or Kaggle"
7. "models with both high downloads and high rating"

### Avanzadas (3)
8. "compare PyTorch vs TensorFlow by average downloads"
9. "count models by task category"
10. "models NOT from HuggingFace"

---

## üìà An√°lisis Incluidos

1. **Validaci√≥n Sint√°ctica**
   - PREFIX correcto
   - Estructura SELECT/WHERE
   - No operaciones peligrosas (DELETE, DROP)

2. **RAG Performance**
   - Ejemplos m√°s recuperados
   - Relevancia de retrieval
   - Distribuci√≥n por complejidad

3. **Ejecuci√≥n Real**
   - Queries ejecutadas exitosamente
   - N√∫mero de resultados obtenidos
   - Errores de runtime

4. **Confidence Scoring**
   - High: Query completa sin warnings
   - Medium: Query v√°lida con warnings
   - Low: Query inv√°lida o incompleta

---

## üìù Outputs Generados

1. **Visualizaciones**: 4 gr√°ficos de an√°lisis
2. **CSV Export**: `data/text_to_sparql_validation_results.csv`
3. **Logs detallados**: En celdas con print statements

---

## üêõ Troubleshooting

### Error: "ANTHROPIC_API_KEY no configurada"
```bash
export ANTHROPIC_API_KEY='sk-ant-...'
```

### Error: "Archivo RDF no encontrado"
```bash
# Ejecutar notebook 02 primero
jupyter notebook notebooks/02_multi_repository_validation.ipynb
```

### Error: "Module 'llm' not found"
```bash
# Verificar que est√°s en el directorio correcto
cd /home/edmundo/ai-model-discovery
python3 -c "from llm import TextToSPARQLConverter; print('OK')"
```

### Error: "ChromaDB no disponible"
```bash
pip install chromadb
```

---

## üîó Relacionado

- **Notebook 02**: `02_multi_repository_validation.ipynb` (genera el grafo RDF)
- **M√≥dulo LLM**: `llm/text_to_sparql.py` (conversor principal)
- **Base de Conocimiento**: `llm/rag_sparql_examples.py` (17 ejemplos)
- **Documentaci√≥n**: `llm/README_LANGCHAIN_RAG.md` (arquitectura)

---

## ‚úÖ Resultados Esperados

Al finalizar, deber√≠as tener:

- ‚úÖ **Success rate ‚â•70%** (7/10 queries v√°lidas)
- ‚úÖ **4 visualizaciones** de an√°lisis
- ‚úÖ **CSV exportado** con resultados detallados
- ‚úÖ **An√°lisis RAG** mostrando ejemplos relevantes
- ‚úÖ **Queries ejecutadas** contra grafo RDF real

Si el success rate es ‚â•70%, el sistema est√° listo para:
1. Integraci√≥n con SearchEngine
2. Creaci√≥n de notebook 04 (b√∫squeda end-to-end)
3. Implementaci√≥n de evaluaci√≥n (precision/recall)

---

**Estado**: ‚úÖ Notebook completo y listo para ejecutar  
**√öltima actualizaci√≥n**: Enero 2024
# Notebook 03: Text-to-SPARQL Validation

## üéØ Objetivo

Validar el sistema completo de conversi√≥n de lenguaje natural a SPARQL usando:
- **TextToSPARQLConverter** con LangChain
- **RAG** con ChromaDB (17 ejemplos)
- **Grafo RDF** con 175 modelos de IA

---

## üìã Contenido del Notebook

### 1. Setup (Celdas 1-3)
- Imports y configuraci√≥n de paths
- Cargar m√≥dulo `llm` con conversor
- Verificaci√≥n de dependencias

### 2. Cargar Grafo RDF (Celda 4)
- Cargar `data/multi_repository_kg.ttl` (del notebook 02)
- Estad√≠sticas: 5,829 triples, 175 modelos
- Namespace DAIMO configurado

### 3. Verificar Base de Conocimiento (Celda 5)
- Cargar 17 ejemplos SPARQL
- Distribuci√≥n por complejidad (basic/intermediate/advanced)
- Mostrar ejemplos de cada nivel

### 4. Inicializar Conversor (Celda 6)
- Verificar `ANTHROPIC_API_KEY`
- Crear `TextToSPARQLConverter(use_rag=True)`
- Configurar RAG con ChromaDB

### 5. Test Queries (Celdas 7-8)
- **10 queries de prueba**:
  - 4 b√°sicas (filtrado, ordenamiento)
  - 3 intermedias (multi-criterio)
  - 3 avanzadas (agregaciones, negaci√≥n)

### 6. Ejecutar Conversiones (Celda 9)
- Procesar todas las queries
- Almacenar resultados con metadata:
  - SPARQL generado
  - Validaci√≥n (is_valid)
  - Confianza (high/medium/low)
  - Ejemplos RAG recuperados

### 7. An√°lisis de Resultados (Celda 10)
- **Success rate general** (target: ‚â•70%)
- **Success rate por complejidad**
- **Distribuci√≥n de confianza**
- Listar queries inv√°lidas

### 8. An√°lisis RAG (Celda 11)
- Top ejemplos m√°s recuperados
- Distribuci√≥n de ejemplos por complejidad de query
- Verificar relevancia de RAG retrieval

### 9. Ejecuci√≥n contra RDF (Celda 12)
- Ejecutar queries v√°lidas en el grafo
- Contar resultados obtenidos
- Detectar errores de ejecuci√≥n

### 10. Ejemplo Detallado (Celda 13)
- Demo completa de conversi√≥n + ejecuci√≥n
- Mostrar SPARQL generado
- Mostrar primeros 5 resultados del grafo

### 11. Visualizaciones (Celda 14)
- 4 gr√°ficos:
  1. Success rate general (bar chart)
  2. Success rate por complejidad (bar chart)
  3. Distribuci√≥n de confianza (bar chart)
  4. Top 5 ejemplos RAG (horizontal bar)

### 12. Resumen Final (Celda 15)
- M√©tricas consolidadas
- Evaluaci√≥n vs objetivo (70%)
- Pr√≥ximos pasos

### 13. Export (Celda 16)
- Guardar resultados en CSV
- Archivo: `data/text_to_sparql_validation_results.csv`

---

## üöÄ C√≥mo Ejecutar

### Pre-requisitos

1. **Instalar dependencias LangChain + RAG**:
   ```bash
   cd /home/edmundo/ai-model-discovery
   ./llm/install_langchain.sh
   ```

2. **Configurar API key**:
   ```bash
   export ANTHROPIC_API_KEY='tu-api-key-aqui'
   ```

3. **Verificar grafo RDF existe**:
   ```bash
   ls -lh data/multi_repository_kg.ttl
   # Si no existe, ejecutar notebook 02 primero
   ```

### Ejecutar Notebook

```bash
# Opci√≥n 1: Jupyter Notebook
jupyter notebook notebooks/03_text_to_sparql_validation.ipynb

# Opci√≥n 2: JupyterLab
jupyter lab notebooks/03_text_to_sparql_validation.ipynb

# Opci√≥n 3: VS Code
# Abrir directamente en VS Code con extensi√≥n de Jupyter
```

---

## üìä M√©tricas Esperadas

| M√©trica | Target | Descripci√≥n |
|---------|--------|-------------|
| **Success Rate** | ‚â•70% | % de queries v√°lidas |
| **Basic Success** | ‚â•90% | Queries b√°sicas v√°lidas |
| **Intermediate Success** | ‚â•80% | Queries intermedias v√°lidas |
| **Advanced Success** | ‚â•70% | Queries avanzadas v√°lidas |
| **High Confidence** | ‚â•60% | Queries con confianza alta |
| **Execution Rate** | ‚â•90% | Queries que ejecutan sin error |

---

## üîç Test Queries Incluidas

### B√°sicas (4)
1. "show me the most popular models"
2. "computer vision models"
3. "models from HuggingFace"
4. "NLP models with high downloads"

### Intermedias (3)
5. "PyTorch models with high downloads"
6. "NLP models from HuggingFace or Kaggle"
7. "models with both high downloads and high rating"

### Avanzadas (3)
8. "compare PyTorch vs TensorFlow by average downloads"
9. "count models by task category"
10. "models NOT from HuggingFace"

---

## üìà An√°lisis Incluidos

1. **Validaci√≥n Sint√°ctica**
   - PREFIX correcto
   - Estructura SELECT/WHERE
   - No operaciones peligrosas (DELETE, DROP)

2. **RAG Performance**
   - Ejemplos m√°s recuperados
   - Relevancia de retrieval
   - Distribuci√≥n por complejidad

3. **Ejecuci√≥n Real**
   - Queries ejecutadas exitosamente
   - N√∫mero de resultados obtenidos
   - Errores de runtime

4. **Confidence Scoring**
   - High: Query completa sin warnings
   - Medium: Query v√°lida con warnings
   - Low: Query inv√°lida o incompleta

---

## üìù Outputs Generados

1. **Visualizaciones**: 4 gr√°ficos de an√°lisis
2. **CSV Export**: `data/text_to_sparql_validation_results.csv`
3. **Logs detallados**: En celdas con print statements

---

## üêõ Troubleshooting

### Error: "ANTHROPIC_API_KEY no configurada"
```bash
export ANTHROPIC_API_KEY='sk-ant-...'
```

### Error: "Archivo RDF no encontrado"
```bash
# Ejecutar notebook 02 primero
jupyter notebook notebooks/02_multi_repository_validation.ipynb
```

### Error: "Module 'llm' not found"
```bash
# Verificar que est√°s en el directorio correcto
cd /home/edmundo/ai-model-discovery
python3 -c "from llm import TextToSPARQLConverter; print('OK')"
```

### Error: "ChromaDB no disponible"
```bash
pip install chromadb
```

---

## üîó Relacionado

- **Notebook 02**: `02_multi_repository_validation.ipynb` (genera el grafo RDF)
- **M√≥dulo LLM**: `llm/text_to_sparql.py` (conversor principal)
- **Base de Conocimiento**: `llm/rag_sparql_examples.py` (17 ejemplos)
- **Documentaci√≥n**: `llm/README_LANGCHAIN_RAG.md` (arquitectura)

---

## ‚úÖ Resultados Esperados

Al finalizar, deber√≠as tener:

- ‚úÖ **Success rate ‚â•70%** (7/10 queries v√°lidas)
- ‚úÖ **4 visualizaciones** de an√°lisis
- ‚úÖ **CSV exportado** con resultados detallados
- ‚úÖ **An√°lisis RAG** mostrando ejemplos relevantes
- ‚úÖ **Queries ejecutadas** contra grafo RDF real

Si el success rate es ‚â•70%, el sistema est√° listo para:
1. Integraci√≥n con SearchEngine
2. Creaci√≥n de notebook 04 (b√∫squeda end-to-end)
3. Implementaci√≥n de evaluaci√≥n (precision/recall)

---

**Estado**: ‚úÖ Notebook completo y listo para ejecutar  
**√öltima actualizaci√≥n**: Enero 2024

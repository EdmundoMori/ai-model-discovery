# TensorFlow Hub - Gu√≠a de Configuraci√≥n

## üìã Informaci√≥n General

**Repositorio**: TensorFlow Hub (https://tfhub.dev)  
**API**: REST API p√∫blica  
**Autenticaci√≥n**: No requerida (API p√∫blica)  
**Formato**: JSON  
**Documentaci√≥n**: https://www.tensorflow.org/hub

## üîç Investigaci√≥n del API

### Endpoints Disponibles

TensorFlow Hub no tiene un API REST oficial documentado, pero expone datos a trav√©s de:

1. **tfhub.dev JSON feeds**
   - Lista de modelos: `https://tfhub.dev/s?subtype=module,placeholder`
   - Metadata individual: No hay endpoint directo

2. **tensorflow_hub Python Package**
   - B√∫squeda program√°tica de modelos
   - Descarga y uso de modelos
   - Metadata extracci√≥n

### Caracter√≠sticas √önicas

- **Formato TF SavedModel**: Modelos optimizados para TensorFlow
- **Categor√≠as espec√≠ficas**: Text, Image, Video, Audio
- **Publishers verificados**: Google, DeepMind, etc.
- **Versioning**: Modelos versionados con URLs √∫nicas
- **Colecciones**: Agrupaciones tem√°ticas de modelos

## üöÄ Instalaci√≥n

### Opci√≥n 1: tensorflow-hub Package (Recomendado)

```bash
# Activar entorno virtual
cd /home/edmundo/ai-model-discovery
source .venv/bin/activate

# Instalar tensorflow-hub
pip install tensorflow-hub

# Verificar instalaci√≥n
python3 -c "import tensorflow_hub as hub; print('‚úÖ TensorFlow Hub instalado')"
```

### Opci√≥n 2: Web Scraping (Alternativa)

```bash
# Instalar beautifulsoup4 y requests
pip install beautifulsoup4 requests

# Verificar instalaci√≥n
python3 -c "from bs4 import BeautifulSoup; import requests; print('‚úÖ Dependencies OK')"
```

## üìä M√©todo de Recolecci√≥n

### Estrategia: Web Scraping del Sitio P√∫blico

Dado que TensorFlow Hub no tiene un API REST documentado, usaremos scraping del sitio:

```python
import requests
from bs4 import BeautifulSoup
import json

# URL base
BASE_URL = "https://tfhub.dev"

# Obtener listado de modelos
response = requests.get(f"{BASE_URL}/s?subtype=module,placeholder")
soup = BeautifulSoup(response.content, 'html.parser')

# Extraer URLs de modelos
model_links = soup.find_all('a', {'class': 'devsite-result-item-link'})

# Para cada modelo, obtener metadata
for link in model_links:
    model_url = BASE_URL + link['href']
    # Fetch model page and extract metadata
```

### Metadata Disponible

De cada p√°gina de modelo se puede extraer:

- **Handle**: URL √∫nica del modelo (ej: `tensorflow/efficientnet/b0/feature-vector/1`)
- **Publisher**: Organizaci√≥n/autor (ej: `google`, `tensorflow`)
- **Architecture**: Tipo de modelo (ej: `EfficientNet`, `BERT`)
- **Task/Domain**: Clasificaci√≥n, detecci√≥n, embeddings, etc.
- **Framework**: TensorFlow version
- **Dataset**: Dataset de entrenamiento
- **Description**: Descripci√≥n textual
- **License**: Tipo de licencia
- **Download Count**: No disponible p√∫blicamente
- **Upload Date**: Fecha de publicaci√≥n

## üó∫Ô∏è Mapeo Propuesto

### TensorFlow Hub ‚Üí StandardizedModel

| Campo TF Hub | StandardizedModel | Notas |
|--------------|-------------------|-------|
| `handle` | `id` | Formato: `publisher/model/version` |
| `handle` | `title` | Nombre legible del handle |
| `publisher` | `author` | Organizaci√≥n/autor |
| `description` | `description` | Descripci√≥n del modelo |
| `upload_date` | `created_at` | Fecha de publicaci√≥n |
| `upload_date` | `last_modified` | Misma fecha (sin updates) |
| N/A | `downloads` | No disponible, usar 0 |
| N/A | `likes` | No disponible, usar 0 |
| `framework` | `library` | Siempre "tensorflow" |
| `architecture` | `architectures` | Lista con arquitectura |
| `task` | `task` | Clasificaci√≥n, embedding, etc. |
| `license` | `license` | Tipo de licencia |
| `dataset` | N/A | En extra_metadata |
| N/A | `source` | Valor fijo: `"tfhub"` |

### Campos en extra_metadata

| Campo | Descripci√≥n |
|-------|-------------|
| `handle` | URL completa del modelo |
| `tfhub_url` | URL web del modelo |
| `publisher` | Organizaci√≥n publicadora |
| `architecture` | Arquitectura del modelo |
| `task_type` | Tipo de tarea (classification, etc.) |
| `dataset` | Dataset de entrenamiento |
| `tf_version` | Versi√≥n de TensorFlow requerida |
| `input_shape` | Shape esperado de entrada |
| `output_shape` | Shape de salida |
| `collection` | Colecci√≥n a la que pertenece |

## üîó Mapeo RDF Espec√≠fico

### Triples Adicionales (map_to_rdf)

```python
# Publisher ‚Üí dcterms:publisher
if publisher:
    <model_uri> dcterms:publisher <publisher_literal> .

# Architecture ‚Üí daimo:architecture
if architecture:
    <model_uri> daimo:architecture <architecture_literal> .

# Task Type ‚Üí daimo:task
if task_type:
    <model_uri> daimo:task <task_literal> .

# Dataset ‚Üí daimo:trainedOn
if dataset:
    <model_uri> daimo:trainedOn <dataset_literal> .

# TF Version ‚Üí daimo:framework
if tf_version:
    <model_uri> daimo:framework <tf_version_literal> .
```

## üìä Estad√≠sticas Esperadas

Basado en tfhub.dev (Enero 2026):

- **Total de modelos**: ~2,500+
- **Publishers**: ~50 (Google, TensorFlow, DeepMind, etc.)
- **Categor√≠as principales**:
  - Text: ~800 modelos
  - Image: ~1,200 modelos
  - Video: ~200 modelos
  - Audio: ~150 modelos
  - Other: ~150 modelos

## üéØ Decisiones de Dise√±o

### 1. Web Scraping vs API

**Decisi√≥n**: Usar web scraping con rate limiting.

**Justificaci√≥n**:
- No hay API REST p√∫blica documentada
- El sitio es p√∫blico y accesible
- Implementar caching para minimizar requests
- Rate limiting de 1 request/segundo

### 2. handle como ID

**Decisi√≥n**: Usar el "handle" completo como ID.

**Justificaci√≥n**:
- Es √∫nico y versionado
- Formato: `publisher/model/version`
- Ejemplo: `google/bert_uncased_L-12_H-768_A-12/1`

### 3. downloads = 0

**Decisi√≥n**: No hay m√©trica de downloads p√∫blica.

**Justificaci√≥n**:
- TensorFlow Hub no expone contadores de descarga
- Podr√≠amos inferir popularidad por collections
- Por ahora usar 0 para consistencia

### 4. Framework fijo

**Decisi√≥n**: Siempre usar "tensorflow" como framework.

**Justificaci√≥n**:
- Todos los modelos son para TensorFlow
- Puede incluir versi√≥n espec√≠fica en extra_metadata

## üêõ Limitaciones Conocidas

1. **Sin API oficial**: Dependemos de scraping, puede romperse con cambios en el sitio
2. **Sin m√©tricas de uso**: No hay downloads, likes, o popularidad
3. **Rate limiting manual**: Debemos implementar delays para evitar bloqueos
4. **Metadata incompleto**: Algunos campos pueden no estar disponibles
5. **Sin b√∫squeda avanzada**: Filtrado limitado en el sitio

## üîÆ Mejoras Futuras

1. **Caching agresivo**: Guardar modelos localmente para reducir scraping
2. **Metadata enriquecido**: Extraer info de collections y tasks
3. **Popularidad inferida**: Usar presencia en collections como proxy
4. **Monitoreo de cambios**: Detectar nuevos modelos peri√≥dicamente
5. **TensorFlow Hub Search API**: Si se documenta en el futuro

## üìö Referencias

- [TensorFlow Hub](https://tfhub.dev)
- [TensorFlow Hub Python API](https://www.tensorflow.org/hub/api_docs/python/hub)
- [TensorFlow Hub GitHub](https://github.com/tensorflow/hub)
- [Common Saved Model APIs](https://www.tensorflow.org/hub/common_saved_model_apis)

---

**Autor**: GitHub Copilot  
**√öltima actualizaci√≥n**: Enero 2026

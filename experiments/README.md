# Experimentos y Evaluaci√≥n

Este directorio contendr√° los scripts de evaluaci√≥n y an√°lisis para las diferentes fases del proyecto.

## Estado: üìÖ Planificado para Fase 5 (Semana 8)

## Experimentos Planificados

### 1. Evaluaci√≥n de Text-to-SPARQL

**Archivo**: `eval_text_to_sparql.py`

M√©tricas:
- Exactitud sint√°ctica (queries v√°lidas)
- Exactitud sem√°ntica (resultados correctos)
- Cobertura de conceptos
- Tiempo de respuesta

Dataset de prueba:
- 50 consultas naturales anotadas
- Ground truth SPARQL correspondiente

### 2. Comparaci√≥n de M√©todos de B√∫squeda

**Archivo**: `compare_search_methods.py`

Comparar:
- M√©todo 1: Non-federated
- M√©todo 2: Federated
- M√©todo 3: Cross-repository

M√©tricas:
- Precision@K
- Recall@K
- F1-Score
- Latencia

### 3. An√°lisis de Cobertura

**Archivo**: `coverage_analysis.py`

Evaluar:
- % modelos con metadatos completos
- Distribuci√≥n de tareas
- Cobertura de licencias
- Calidad de mapeo a DAIMO

### 4. An√°lisis de Popularidad vs Relevancia

**Archivo**: `popularity_vs_relevance.py`

Investigar:
- Correlaci√≥n entre descargas y relevancia
- Sesgo de popularidad en ranking
- Diversidad de resultados

## Formato de Resultados

Los experimentos generar√°n:

- **CSV/JSON**: Resultados tabulados
- **Gr√°ficos**: Visualizaciones (matplotlib/seaborn)
- **Reportes**: Markdown con an√°lisis

Ejemplo:
```
experiments/
  results/
    eval_text_to_sparql_2026-01-26.json
    coverage_analysis_2026-01-26.csv
  figures/
    precision_recall_curve.png
    task_distribution.png
```

## Benchmarks

### Test Suite para Text-to-SPARQL

Categor√≠as de consultas:
1. **Simple**: Filtro por una propiedad
2. **Compuesta**: M√∫ltiples filtros
3. **Agregaci√≥n**: COUNT, AVG, etc.
4. **Comparativa**: Ranking, TOP-K
5. **Provenance**: Modelos derivados, datasets

Ejemplo:
```json
{
  "id": 1,
  "query_nl": "Top 5 modelos de clasificaci√≥n de im√°genes con licencia MIT",
  "query_sparql": "PREFIX daimo: ...",
  "difficulty": "medium",
  "expected_results": 5
}
```

## Reproducibilidad

Todos los experimentos incluir√°n:
- Seed fijo para aleatoriedad
- Versi√≥n de dependencias (Poetry lock)
- Configuraci√≥n expl√≠cita en YAML
- Scripts de ejecuci√≥n automatizados

## Referencias Acad√©micas

Los resultados se documentar√°n siguiendo est√°ndares de:
- ACL (para NLP/LLM)
- ISWC (para Semantic Web)
- MLSys (para infraestructura ML)
